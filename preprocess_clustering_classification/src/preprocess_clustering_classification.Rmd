---
title: 'Preprocessing, clustering and classification in R'
output:
  html_notebook: 
    toc: yes
    toc_float: yes
    number_sections: yes
---

# Creation

## 20 NewsGroup dataset

The [20 NewsGroup dataset](http://qwone.com/~jason/20Newsgroups/) is a popular NLP benchmark collection of approximately 20, 000 newsgroup documents, partitioned in 20 predefined thematic newsgroups. This tutorial is completed with the “20news-bydate.tar.gz” compressed file: you can find its link in the middle of the webpage. Unzip the compressed file: training and testing folders are created. Each of them contains 20 folders: each containing the text documents belonging to one newsgroup. We focus the tutorial on training “sci.electronics” and “talk.religion.misc” groups-folders. Focus in ‘training folders’ of “sci.electronics” and “talk.religion.misc”. We are going to treat them as documents belonging to two different classes-topics: science or religion.

Help for a function in R is obtained by invoking `?VCorpus` or `help(VCorpus)`. A good way to consult the different functions (and parameters) of each package is the https://www.rdocumentation.org/ community. In our case, the functions of the tm package, grouped alphabetically, can be consulted in https://www.rdocumentation.org/packages/tm/. I consider it as **mandatory** to develop your own text-preprocessing pipeline for your text and corpus. Searching in the web for the specific function, followed by the R term, usually produces good helping results.

We start by **reading the documents of each subdirectory** (annotated document class) and loading them in a volatile corpora structure. Function inspect displays detailed information of the corpus. It is first needed to fix R in the working directory which saves the data-corpus, as previously exposed.

```{r}
library(tm)
sci.elec <- VCorpus(DirSource("../../text_processing/data/sci.electronics"), readerControl = list(language = "en"))
talk.religion <- VCorpus(DirSource("../../text_processing/data/talk.religion.misc"), readerControl = list(language = "en"))
sci.elec # dimension of the corpus
inspect(sci.elec[1]) # first document of the corpus, or sci.elec.train[[1]]
inspect(sci.elec[1:3]) # first three documents of the corpus
```

## Retrieve text and create a corpus from html pages

It is also easy to retrieve text in HTML format from the web by means of R functionalities. A simple example can be to retrieve the “Indemnifications” sections of a “Terms of Services” webpage of GoogleAnalytics. The readLines command outputs a vector of character strings, each component storing a line. The text of interest exists from line 302 through 325. The HTML tags are removed. Now, by means of the tm package, the vector of character strings can be converted into a corpus of text using consecutively the VectorSource and the VCorpus functions.

```{r}
indemnifications.HTML.page <- readLines("https://www.google.com/analytics/terms/us.html")
length(indemnifications.HTML.page)
text.data <- indemnifications.HTML.page[302:325] # 24 lines of desired text
text.data <- gsub(pattern = "<p>", replacement = "", x = text.data)
text.data <- gsub(pattern = "</p>", replacement = "", x = text.data)
text.data <- gsub(pattern = "</h2>", replacement = "", x = text.data)
text.data <- gsub(pattern = "<h2>", replacement = "", x = text.data)
text.data
text.data <- VectorSource(text.data) # interpreting each element of the vector as a document
text.data.corpus <- VCorpus(text.data) # from document list to corpus
```

## Retrieve text and create a corpus from files (stored in disk)

Other common practice to retrieve text is by means of the examples stored by R packages. In this case, when installing the tm package a set of document collections are stored in hard disk. Among them, a subset of the popular “Reuters-21578” collection. The system.file function points the subdirectory-tree that has the documents of interest in our package. When reading the text, an specific XML reader developed by the community, known as “readReut21578XMLasPlain”, is needed.

```{r}
reut21578 <- system.file("texts", "crude", package = "tm")
reut21578
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
```

## Retrieve text and create an annotated corpus from Twitter: using rtweet or twitteR packages

A fashion way to retrieve text is via Twitter posts. The twitteR library provides access to Twitter data. Twitter marks its use as the ‘official’ way to download its tweets. (In case you have problems with twitter package, I show you an alternative at the end of this subsection)

Second, I try to explain you the ‘official’ way offered by the twitteR library. Twitter API requires identification-authentification: follow these instructions: http://thinktostart.com/twitter-authentification-with-r/.

Pay attention, the current Twitter’s link to create Twitter applications is https://developer.twitter.com/en/apps. You need to be logged. It is needed to “create a new app”: this will provide you a set of 4 items related to the application called “consumerKey”, “consumerSecret”, “accessToken” and “accessSecret”. Both ‘accessToken” and “accessSecret” need to be activated after receiving the “consumerKey” and “consumerSecret”. Four parameters need to be used in the final authentification function call in R, setup twitter oauth().

An alternative explanation, by twitter, of the exposed process. The following https://dev.twitter.com/oauth/overview/application-owner-access-tokens also explains the exposed Twitter’s identification-authentification process, which hangs from the general Twitter Developer Documentation https://developer.twitter.com/en/docs/authentication/overview

At a first glance, this process can be felt as cumbersome-complex. Be patient, and after several trials sure that you will be able to authentificate. If the process fails, try installing the httr and base64enc packages; if the error continues, install the twitteR package from GitHub, in this way:

```{r}
library(twitteR)
source("twitter_oauth.R")
setup_twitter_oauth(consumer_key, consumer_secret, access_key, access_secret)
```

While it is true that you can find many short guides to establish the connection with twitter in order to start downloading its text, I just offer you another pointer: https://www.kdnuggets.com/2017/11/extracting-tweets-r.html

Once the authentification is done, tweets of any user or hashtag can be retrieved and converted to a corpus. The functions provided by the twitteR package evolve continuously and sure that you find interesting functions for your NLP analysis objectives.

Just an idea. To further apply machine learning techniques to learn supervised classifiers, our corpus needs to have documents with different annotations-types: confronting two antagonistic users and downloading their tweets, an annotated, binary corpus can be constructed. After preprocessing and converting the corpus to a data frame format, supervised classification techniques can be applied over it.

```{r}
# consult the different ways to retrieve tweets: from an user or from a hashtag
?userTimeline
?searchTwitteR
LadyGagaTweets <- userTimeline(user = "ladygaga", n = 100)
CristianoRonaldoTweets <- userTimeline(user = "Cristiano", n = 100)
# beerTweets = searchTwitteR(searchString = "#beer", n=100)
length(LadyGagaTweets)
LadyGagaTweets[1:4]
# convert each tweet to a data frame and combine them by rows
LadyGagaTweetsDataFrames <- do.call("rbind", lapply(LadyGagaTweets, as.data.frame))
CristianoRonaldoTweetsDataFrames <- do.call("rbind", lapply(CristianoRonaldoTweets, as.data.frame))
# consult the different attributes of these object using \£ symbol
LadyGagaTweetsDataFrames$text
# combine both frames in a single, binary, annotated set
annotatedTweetsDataFrames <- rbind(LadyGagaTweetsDataFrames, CristianoRonaldoTweetsDataFrames)
# interpreting each element of the annotated vector as a document
annotatedDocuments <- VectorSource(annotatedTweetsDataFrames$text)
# convert to a corpus: supervised classification to be applied in future steps
annotatedCorpus <- VCorpus(annotatedDocuments)
```

In case you had problems with twitter package, an attractive and ‘easy-to-use’ alternative to Twitter’s ‘official rules’ is based on the use of the rtweet package. The [following link](https://github.com/ropensci/rtweet) seems to be a ‘more updated’ package. I think it is needed to have a Twitter account (username and password). This [set of slides](https://mkearney.github.io/nicar_tworkshop/) offers an easy-to-follow tutorial, showing the pipeline that you need.

# Preprocessing

## Clean text

If you’re working with tweets, use the following line of code to clean your text.

```{r}
gsub("https\\S*", "", annotatedCorpus$text) 
gsub("@\\S*", "", annotatedCorpus$text) 
gsub("amp", "", annotatedCorpus$text) 
gsub("[\r\n]", "", annotatedCorpus$text)
gsub("[[:punct:]]", "", annotatedCorpus$text)
```

## Apply transformations

Transformations operators to the corpus are applied via `tm_map` function, which applies (maps) a function to all elements of the corpus. As the same transformations will be applied to both “science” and “religion” newsgroups, both corpus are merged using base function `c()`: this `c()` operator concatenates objects. This operation raises a collection of 968 documents. Function tm map applies transformations to corpus objects. The list of available transformations can be obtained consulting the help of ?getTransformations. Function content transformer is used to apply customized transformations. We apply several transformations. As NLP practitioner: consult the help and parameters of each transformation’s functions, of course, use them in a different and richer way then me.

```{r}
sci.rel <- c(sci.elec, talk.religion) # merge, concatenate both groups-corpuses
sci.rel.trans <- tm_map(sci.rel, removeNumbers)
sci.rel.trans <- tm_map(sci.rel.trans, removePunctuation)
sci.rel.trans <- tm_map(sci.rel.trans, content_transformer(tolower)) # convert to lowercase
stopwords("english") # list of english stopwords
sci.rel.trans <- tm_map(sci.rel.trans, removeWords, stopwords("english"))
sci.rel.trans <- tm_map(sci.rel.trans, stripWhitespace)
library(SnowballC) # to access Porter's word stemming algorithm
sci.rel.trans <- tm_map(sci.rel.trans, stemDocument)
```

## Create a document-term matrix

After corpus set transformation, a common approach in text mining is to **create a document-term matrix** from a corpus. Its transpose operator creates a term-document matrix. This document-term matrix is the starting point to apply machine-learning modelization techniques such as classification, clustering, etc. Different operations can be applied over this matrix. We can obtain the terms that occur at least, let say, 15 times; or consult the **terms that associate** with at least, for example, by a 0.7 correlation degree with the term “young”.

```{r}
sci.rel.dtm <- DocumentTermMatrix(sci.rel.trans)
dim(sci.rel.dtm)
inspect(sci.rel.dtm[15:25, 1040:1044]) # inspecting a subset of the matrix
findFreqTerms(sci.rel.dtm, 15)
findAssocs(sci.rel.dtm, term = "young", corlimit = 0.7)
```

After all, it is easy no note the huge degree of sparsity of this matrix: a low amount of non-zero elements. Thus, one of the most important operations is to remove sparse terms, i.e., terms occurring in very few documents. The `sparse` parameter in the `removeSparseTerms` function refers to the maximum sparseness allowed: the smaller its proportion, fewer terms (but more common) will be retained. A “trial and error” approach will finally return a proper number of terms. This matrix will be the starting point for building further machine learning models (in the next tutorial).

```{r}
sci.rel.dtm.70 <- removeSparseTerms(sci.rel.dtm, sparse = 0.7)
sci.rel.dtm.70 # or dim(sci.rel.dtm.70)
# note that the term-document matrix needs to be transformed (casted)
# to a matrix form in the following barplot command
barplot(as.matrix(sci.rel.dtm.70),
    xlab = "terms", ylab = "number of occurrences",
    main = "Most frequent terms (sparseness=0.7)"
)
sci.rel.dtm.80 <- removeSparseTerms(sci.rel.dtm, sparse = 0.8)
sci.rel.dtm.80
barplot(as.matrix(sci.rel.dtm.80),
    xlab = "terms", ylab = "number of occurrences",
    main = "Most frequent terms (sparseness=0.8)"
)
sci.rel.dtm.90 <- removeSparseTerms(sci.rel.dtm, sparse = 0.9)
sci.rel.dtm.90
barplot(as.matrix(sci.rel.dtm.90),
    xlab = "terms", ylab = "number of occurrences",
    main = "Most frequent terms (sparseness=0.9)"
)
```

## Convert the "DocumentTerm matrix" to WEKA's *.arff data format

Different functionalities of R allow to convert these matrices to the file format demanded by other software tools. For example, the function write.arff of the foreign package converts a data matrix or data frame to the well-known arff (“attribute relation format file”) of WEKA software. Note that a class vector is appended to the document-term matrix, labeling the type of each document. In my case (total numbers can be different in your corpus), the first 591 documents cover the “science-electronics” newgroup

```{r}
# convert corpus to dataFrame format
data <- data.frame(as.matrix(sci.rel.dtm.90))
# create the type vector to be appended
type <- c(rep("science", 591), rep("religion", 377))
# install the package for apply the conversion function
library(foreign)
write.arff(cbind(data, type), file = "../data/sci.rel.dtm.90.arff")
# you can try to open the new .arff file in WEKA...
```

# Visualization

## Build a wordcloud

Before starting learning the exposed machine learning models, let’s build a wordcloud with the following package [3]. Its `wordcloud()` command needs the list of words and their frequencies as parameters. As the words appear in columns in the document-term matrix, the `colSums` command is used to calculate the word frequencies. In order to complete the needed calculations, note that the term-document matrix needs to be transformed (casted) to a matrix form with the `as.matrix` cast-operator. It is built for the “talk.religion” newsgroup: it covers the 591-968 range of samples-documents (rows) in the document-term matrix. Let’s play yourself with the options-parameters of the wordcloud function: it offers many options to become the wordcloud more attractive, discover by yourself.

```{r}
library(wordcloud)
# calculate the frequency of words and sort in descending order.
wordFreqs <- sort(colSums(as.matrix(sci.rel.dtm.90)[591:968, ]), decreasing = TRUE)
wordcloud(words = names(wordFreqs), freq = wordFreqs, max.words=200, random.order=FALSE, colors=brewer.pal(8, "Dark2"))

wordFreqs <- sort(colSums(as.matrix(sci.rel.dtm.90)[1:590, ]), decreasing = TRUE)
wordcloud(words = names(wordFreqs), freq = wordFreqs, max.words=200, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
```

# Clustering

## Clustering of words with similar patterns of occurrences across documents

We try to find clusters of words with hierarchical clustering, a popular clustering techniques which builds a dendogram to iteratively group pairs of similar objects. To do so, a matrix which has removed sparse is needed: the starting point is the 0.8 sparseness-value matrix. After the application of the matrix-casting operator, number of occurrences are scaled: first column (term) mean is substracted, and then this is divided by its standard deviation. It is needed to calculate the distance between pairs of objects (terms): these are saved in `distMatrix`. The `dist` operator performs this calculation between pairs of rows of the provided matrix. As terms appear in the columns of the document-term matrix (`sci.rel.dtm.80`), it is needed to be transposed by means of the `t` operator. The clustering-dendogram is built with the `hclust` operator. It needs as input the calculated distance matrix between pairs of terms and a criteria to decide which pair of clusters to be consecutively joined in the bottom-up dendogram. In this case, the “complete” criteria takes into account the maximum distance between any pair of objects (terms) of both clusters to be merged. Heigth in the dendogram denotes the *distance* between a merged pair of clusters.

```{r}
distMatrix <- dist(t(scale(as.matrix(sci.rel.dtm.80))))
termClustering <- hclust(distMatrix, method = "complete")
plot(termClustering)
```

Instead of hierarchical clustering and following a similar set of functions, the `fpc` package allows to construct a `k-means` clustering.

Another type of popular NLP machine-learning analysis is to construct clusters of similar documents based on the frequencies of word occurrences.

# Classification

Our objective is to learn a classifier-model which, based on terms occurrences, predicts the type-topic (“science-electronics” or “religion”) of future documents (post-new, in the case of newsgroups). We have a two-class problem.

## Concatenate the annotation column: type of training documents

The 0.9 sparseness value document-term matrix is our starting point. We first need to append the class (document type) vector as the last column of the matrix: the first 591 documents cover the “science-electronics” newgroup.

```{r}
dim(sci.rel.dtm.90)
type <- c(rep("science", 591), rep("religion", 377)) # create the type vector
sci.rel.dtm.90 <- cbind(sci.rel.dtm.90, type) # append
dim(sci.rel.dtm.90) # consult the updated number of columns
```

This new matrix is the starting point for any software specialized on supervised classification. However, it is needed to concatenate “matrix” and “data.frame” casting operations. The name of the last column is updated.

```{r}
sci.rel.dtm.90.ML.matrix <- as.data.frame(as.matrix(sci.rel.dtm.90))
colnames(sci.rel.dtm.90.ML.matrix)[119] <- "type"
```

The different columns of an object can be accessed by typing the tab after the object name and the $ symbol.

## Classification by R-package `caret`, covering a classic data-mining analysis pipeline

The `caret` [4, 5] package is the reference tool for building supervised classification and regression models in R. The following shows the current top machine learning packages in `R`: https://www.kdnuggets.com/2017/02/top-r-packages-machine-learning.html. `caret` package covers all the steps of a classic pipeline: data preprocessing, model building, accuracy estimation, prediction of the type of new samples, and statistical comparision between the performance of different models. Another similar package is `mlr3`. If you are interested, you can find an interesting tutorial: https://mlr3.mlr-org.com/.

Very useful: the cheatsheet of caret: https://github.com/CABAH/learningRresources/blob/main/cheatsheets/caret.pdf. Its principal functions illustrated in a single page.

## Create a "Train-Test" partition for classifier validation

Before learning a classification model it is needed to define the subsets of samples (documents) to train and test the it. The `createDataPartition` produces a train-test partition of our corpus of 968 documents. This will be maintained during the whole pipeline of analysis. Test samples won’t be used for any modeling decision:only to predict their class and create a confusion matrix. Consult the parameters of `createDataPartition`, as well as other two functions with similar purposes, `createFolds` and `createResample`. A list of randomly sampled numbers (object `inTrain`), as index numbers, is used to partition the whole corpus in two `R` objects.

```{r}
library(caret)
set.seed(107) # a random seed to enable reproducibility
inTrain <- createDataPartition(y = sci.rel.dtm.90.ML.matrix$type, p = .75, list = FALSE)
str(inTrain)
training <- sci.rel.dtm.90.ML.matrix[inTrain, ]
testing <- sci.rel.dtm.90.ML.matrix[-inTrain, ]
nrow(training)
```

## Selection of supervised classification algorithms 

We now can start training and testing different supervised classification models. train function implements the building process. Check its parameters: among them, we highlight the following:

* `preProcess` parameter defines the preprocessing steps to be applied. They are popular with classic numeric variables, such as imputation of missing values, centering and scaling, etc. As it was shown in the previous tutorial, NLP datasets have their own preprocessing tools. They are not going to be applied in our dataset.

* `trControl` parameter defines the method to estimate the error of the classifier. It is defined by means of the application of the `trainControl` function. This allows the use of different performance estimation procedures such as k-fold cross-validation, bootstrapping, etc. We apply a 10-fold cross-validation, repeated 3 times.

* `method` parameter fixes the type of classification algorithm to be learned. The wide list of algorithms (and its parameters) covered by `caret` can be found in https://topepo.github.io/caret/train-models-by-tag.html. Taking into account the large dimensionality of classic NLP datasets, the use of classifiers capable to deal with this characteristic is highly recommended. In this tutorial, Linear support vector machine (SVM) and k-nearest neighbour (K-NN) models are learned and validated.

* `metric` parameter fixes the score to assess-validates the goodness of each model. Apart from the ROC metric used in this tutorial, a large set of metrics is offered: Accuracy (percentage of correct classification), kappa, Sens (Sensitivity), Specificity (Spec), RMSE in the case of regression problems... I have not found a list with all the metrics offered by `caret`, but consulting the help of the package will give you a broad idea.

Take into account the following facts about the model-training step:

* the expression `type ~` is quite popular in R to denote the specific variable to be predicted, followed by the set of predictors. A point indicates that the rest of variables are used as predictors.

* together with the estimation of the percentage recognition (accuracy), the value of the *Kappa statistic* is shown. It is a popular score in NLP studies. Its definition is not trivial in the context of supervised classification. Roughly, this score compares the “observed” accuracy of the learned classifier with respect to a random classifier: measuring the score difference between our classifier and a random classifier. A larger definition of this metric can be found in http://stats.stackexchange.com/questions/82162/kappa-statistic-in-plain-english.

* while the linear SVM classifier does not have parameters, K-NN has the “number of neighbours” (K) key parameter. By default, changing the value of the parameter, `caret` evaluates 3 models (`tuneLength` equal to 3). The `tuneLength` option of the `train` function fixes the number of values of each parameter to be checked. For example, if the classifier has 2 parameters and the `tuneLength` parameter is not changed, 3 x 3 = 9 models are evaluated. The `tuneGrid` option offers the possibility to select among a set of values to be tuned-tested.

* `caret` supports more than 150 supervised classification and regression algorithms. A small portion of them are learned by means of the software of the package itself. The majority of the algorithms are learned by other `R` packages which are conveniently accessed by `caret`.

## Internal performance estimation in the training partition

```{r}
# fixing the performance estimation procedure
ctrl <- trainControl(method = "repeatedcv", repeats = 3)
svmModel3x10cv <- train(type ~ ., data = training, method = "svmLinear", trControl = ctrl)
svmModel3x10cv
knnModel3x10cv <- train(type ~ ., data = training, method = "knn", trControl = ctrl)
knnModel3x10cv
```
## Tuning of the parameters of the selected classifiers

The training process can still be enriched with extra parameters in the `trainControl` function: `summaryFunction` controls the type of evaluation metrics. In binary classification problems (e.g. “science” versus “religion”) the `twoClassSummary` option displays [area under the ROC curve](http://en.wikipedia.org/wiki/Receiver_operating_characteristic), sensitity-recall and specificity metrics. To do so, it is also needed to activate the `classProbs` option which saves the probability that the classifier assigns to each sample belonging to each class-value.

```{r}
library(pROC)
ctrl <- trainControl(
    method = "repeatedcv", repeats = 3, classProbs = TRUE,
    summaryFunction = twoClassSummary
)
knnModel3x10cvROC <- train(type ~ .,
    data = training, method = "knn", trControl = ctrl,
    metric = "ROC", tuneLength = 10
)
knnModel3x10cvROC
plot(knnModel3x10cvROC)
```

## Predict the class-type of future unseen-unlabeled texts

In order to predict the class value of unseen documents of the test partition caret uses the classifier which shows the best accuracy estimation of their parameters. Function predict implements this functionality. Consult its parameters. The `type` parameter, by means of its `probs` value, outputs the probability of test each sample belonging to each class (“a-posteriori” probability). On the other hand, the `raw` value outputs the class value with the largest probability. By means of the `raw` option the confusion matrix can be calculated: this crosses, for each test sample, predicted with real class values.

```{r}
svmModelClasses <- predict(svmModel3x10cv, newdata = testing, type = "raw")
confusionMatrix(data = svmModelClasses, testing$type)
```

## Statistical comparison between two classifiers by means of t-test

Can a statistical comparison be performed between the 3x10cv validation results of K-NN and SVM? Note than in our case, due to the 3 repetitions of the 10-fold cross-validation process, there are 30 resampling results for each classifier. First, results of both classifiers are crossed using the `resamples` function. As the `set.seed` did not change, the same paired cross-validation subsets of samples were used for both classifiers. This forces to use a paired t-test to calculate the significance of the differences between both classifiers. A simple plot is drawn, showing the accuracy differences between both models for each of the 30 cross-validation folds: note that svm has a better accuracy than knn for all the cross-validation fold results (Figure 2).

Using the `diff` function over the `resamps` object which saves the “crossing” of both classifiers, we can show a rich output of the performed comparison: each number matters. The output shows, for each metric (area under the ROC curve, sensitivity, specificity), the difference of the mean (positive or negative, following the order in the `resamples` function) between both classifiers. The p-value of the associated t-test is also shown.

The interpretation of the p-value has the key. It is related with the risk of erroneously discarding the nullhypothesis of similarity between compared classifiers, when there is no real difference. Roughly speaking, it can also be interpreted as the degree of similarity between both classifiers. A p-value smaller than 0.05 (or 0.1, depending on your interpretation and threshold) alerts about statistically significant differences between both classifiers, https://en.wikipedia.org/wiki/Statistical_significance. That is, when the risk of erroneusly discarding the hypothesis of similarity between both classifiers is low, we assume that there is a statistically significant difference between classifiers.

```{r}
resamps <- resamples(list(knn = knnModel3x10cv, svm = svmModel3x10cv))
summary(resamps)
xyplot(resamps, what = "BlandAltman")
diffs <- diff(resamps)
summary(diffs)
```

# Bibliography

[1] Ingo Feinerer. tm: Text Mining Package, 2012. R package version 0.5-7.1.

[2] Ingo Feinerer, Kurt Hornik, and David Meyer. Text mining infrastructure in R. Journal of Statistical Software, 25(5):1-54, 3 2008.

[3] Ian Fellows. wordcloud: Word Clouds, 2014. R package version 2.5.

[4] M. Kuhn and K. Johnson. Applied Predictive Modeling. Springer, 2013.

[5] Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer, and the R Core Team. caret: Classification and Regression Training, 2014. R package version 6.0-35.
